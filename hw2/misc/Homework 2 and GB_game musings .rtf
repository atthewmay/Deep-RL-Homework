{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red77\green77\blue77;}
{\*\expandedcolortbl;\csgray\c100000;\csgray\c37475;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 s\
GB_game. Note that I didn\'92t hit a better reward until i went w/ bigger batch size.\
\
Also, i think that most was solved, but there were some with horrible reward of -95 (almost as bad as possible). At that point, the unscaled loss [log_prob(a | s)] was very small (abs(#) was small)! meaning it was so certain to take that action. Then with many more training sessions, the unscaled loss increased in size, representing less certainty, but it didn\'92t break out of the issue. I suspect it would have.\
\
Ideas to try next. Reward it for being in the centermost portion of the board. Reward for moving smoothly:)\
	\cf2 Hmm, with reward at -100 for death, it somehow learns to get a hugely negative reward! That\'92s with penalty for corner sitting. It must be that the -100 makes the gradient too big, and those weights get stuck large, bc it doesn\'92t seem like it would try to learn a negative reward. But maybe the small (-) rewards don\'92t provide enough info somehow\'85 Try taking away the penalty.\
\
Ah, perhaps with the penalty and normalization of reward, the -0.414 of the corner is actually a small (+) # post normalization, and thus even tho abs(reward) is very very (-), the normalized reward shows these values to be desirable?\
\
w/o death penalty, it learns, with some catastrophic forgetting, but overall the unscaled loss seems to increase! I think that means it\'92s often staying in the undecisive middle? And maybe the std of the action choice dist is so big that it moves one way to avoid gettin ghit? I believe for the unscaled-loss to keep climbing like it does, the std of teh dist must be increasing, bc else there\'92s no way you\'92d alwasy sample from a low-prob portion of the dist. That\'92s interesting. The std is trained as a separate variable in TF. And it\'92s not dependent on the input. I wonder if there\'92s a point of no return for the std, like once the dist becomes so flat, you can\'92t make it unflat anymore.\
\
OH! I just realized my entire setup is flawed! This is a distribution, and thus the actions chosen can be greater than max_speed. Wow. There you have it. right now it uses tanh output. if I wanna restrict actions in the current setup, I probably need to do so in the tf graph, as an external cap doesn\'92t reflect an action taken perhaps. IDK tho. Maybe this becomes a discreet problem if done that way. Well the std came down anyway it looks. \cf0 \
\
Learning rate decay.\
\
Add a few more enemies. (for this should also automatically adjust network size, etc.\
\
Try segment-sum network. \
\
Try normal w/o death penalty\
\
try feeding in relative positions and velocities too! free features for a small network.\
\
Try to show the network weights and activations and action distributions in real time. Probs use a smaller board and a smaller network too. }